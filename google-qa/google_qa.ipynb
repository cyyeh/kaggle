{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "google-qa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyyeh/kaggle/blob/master/google-qa/google_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oLbtZU7ouH-",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notbook demonstrates how to train/evaluate/test short answer classification task based on Google Natural Question dataset using BERT.\n",
        "\n",
        "## General Steps to Solve This Problem\n",
        "\n",
        "1. [X] Prepare raw data\n",
        "2. [ ] Transform raw data into BERT compatiple format\n",
        "3. [ ] Add new layers for downstream task on the BERT model\n",
        "4. [ ] Train the model\n",
        "5. [ ] Make inference on new data\n",
        "\n",
        "### References\n",
        "\n",
        "- [進擊的 BERT：NLP 界的巨人之力與遷移學習](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVptgN32lQcp",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-T2r-tyyRwN",
        "colab_type": "code",
        "outputId": "0d31cbaa-4ea0-4243-e0f0-21cf0a026e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl3zrHLT8BvF",
        "colab_type": "code",
        "outputId": "5ac15eb8-c355-47c0-e847-8c0b1ec16d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install transformers # BertModel"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\r\u001b[K     |▊                               | 10kB 24.1MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 276kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 286kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 296kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 307kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 317kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 327kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 337kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 348kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 358kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 368kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 378kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 389kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 399kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 409kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 419kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 430kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 440kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 450kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 460kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 471kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 481kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /tensorflow-2.1.0/python3.6 (from transformers) (2.22.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /tensorflow-2.1.0/python3.6 (from transformers) (1.18.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.2MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 39.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (1.25.8)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from sacremoses->transformers) (1.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=42bf5269d9491331d2ea7d25e96d5191f6792f339fc5c40fb6695449c0e1694f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ehmlmDf73e_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "from transformers import BertTokenizer, TFBertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFjdY_FQHDDZ",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3sbfCf0UXu5",
        "colab_type": "code",
        "outputId": "ed3daa96-aa65-4b82-cde3-bf02a415ed00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOpynsibvdFi",
        "colab_type": "text"
      },
      "source": [
        "Check if training/testing dataset is available in your google drive. If it's not available, you should run code inside the \"Prepare Kaggle Dataset\" section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h3chjosvwEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32a3aa2c-4d78-41ad-bcf6-96395f4bcad4"
      },
      "source": [
        "if os.path.exists('drive/My Drive/simplified-nq-train.csv') and \\\n",
        "os.path.exists('drive/My Drive/simplified-nq-test.csv'):\n",
        "  print(\"Training/testing dataset is available!\")\n",
        "else:\n",
        "  print(\"Training/testing dataset is not found, please run code inside the 'Prepare Kaggle Dataset' section.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training/testing dataset is available!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wycKI_jdbSe2",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Kaggle Dataset for [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyY2eSc6klo-",
        "colab_type": "text"
      },
      "source": [
        "### Download Question Answering Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3M1z5fhKgO3",
        "colab_type": "code",
        "outputId": "6a801f4c-7f2f-4602-fb96-2fb34d6ab5b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"chihyuyeh\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"f21b340fc8082977cbf954c80ad69ae1\" # key from the json file\n",
        "!kaggle competitions download -c tensorflow2-question-answering"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/18.2k [00:00<?, ?B/s]\n",
            "100% 18.2k/18.2k [00:00<00:00, 10.2MB/s]\n",
            "Downloading simplified-nq-train.jsonl.zip to /content\n",
            "100% 4.45G/4.46G [01:21<00:00, 66.6MB/s]\n",
            "100% 4.46G/4.46G [01:21<00:00, 58.9MB/s]\n",
            "Downloading simplified-nq-test.jsonl.zip to /content\n",
            "  0% 0.00/4.78M [00:00<?, ?B/s]\n",
            "100% 4.78M/4.78M [00:00<00:00, 43.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fNclz9Pk16X",
        "colab_type": "text"
      },
      "source": [
        "### Unzip Question Answering Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BXCnu4BeeN5",
        "colab_type": "code",
        "outputId": "c0fceec4-f9f5-4830-ad0e-aa7f9cbc12b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!unzip simplified-nq-train.jsonl.zip\n",
        "!unzip simplified-nq-test.jsonl.zip"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  simplified-nq-train.jsonl.zip\n",
            "  inflating: simplified-nq-train.jsonl  \n",
            "Archive:  simplified-nq-test.jsonl.zip\n",
            "  inflating: simplified-nq-test.jsonl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amvgDGEsItBc",
        "colab_type": "text"
      },
      "source": [
        "### Generate data I need and export it to csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0LCl-QDJr9U",
        "colab_type": "text"
      },
      "source": [
        "check data fields in simplified-nq-train.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdzpA_3_I53u",
        "colab_type": "code",
        "outputId": "6dd3cb3b-0acd-4a7a-a969-b9feb32d1d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('simplified-nq-train.jsonl') as f:\n",
        "  line = f.readline()\n",
        "  json_obj = json.loads(line)\n",
        "  print(json_obj.keys())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['document_text', 'long_answer_candidates', 'question_text', 'annotations', 'document_url', 'example_id'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5cTm5oyLFht",
        "colab_type": "text"
      },
      "source": [
        "Data fields in simplified-nq-train.jsonl\n",
        "- document_text\n",
        "- long_answer_candidates\n",
        "- question_text\n",
        "- annotations\n",
        "- document_url\n",
        "- example_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okamj3hALTWM",
        "colab_type": "text"
      },
      "source": [
        "check data fields in simplified-nq-test.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIr5apSDLXf2",
        "colab_type": "code",
        "outputId": "a6727251-45f3-4c04-a492-9b6c806dd46c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('simplified-nq-test.jsonl') as f:\n",
        "  line = f.readline()\n",
        "  json_obj = json.loads(line)\n",
        "  print(json_obj.keys())"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['example_id', 'question_text', 'document_text', 'long_answer_candidates'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg2hflM7Lgpj",
        "colab_type": "text"
      },
      "source": [
        "Data fields in simplified-nq-test.jsonl\n",
        "- example_id\n",
        "- question_text\n",
        "- document_text\n",
        "- long_answer_candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glPMScs3Lu4o",
        "colab_type": "text"
      },
      "source": [
        "Data fields that are not needed in training data:\n",
        "- document_text\n",
        "- document_url\n",
        "\n",
        "Data fields that are not needed in testing data:\n",
        "- document_text\n",
        "\n",
        "I will remove these data fields in order to reduce memory size for the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QcFuFgEPp1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "LONG_ANSWER_CANDIDATES = 'long_answer_candidates'\n",
        "QUESTION_TEXT = 'question_text'\n",
        "ANNOTATIONS = 'annotations'\n",
        "EXAMPLE_ID = 'example_id'\n",
        "DOCUMENT_TEXT = 'document_text'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77m1C2HRw0Ti",
        "colab_type": "text"
      },
      "source": [
        "Since a short answer exists only if a long answer exists, so we will remove cases where long answers don't exist first.\n",
        "\n",
        "For long answers that don't exist, `start_token`, `candidate_index`, and `end_token` are all -1 in `annotations` of `simplified-nq-train.jsonl`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2OK1dh1lhBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_long_answer_existed(annotations):\n",
        "  long_answer = annotations['long_answer']\n",
        "  return long_answer['start_token'] != -1 \\\n",
        "  and long_answer['candidate_index'] != -1 \\\n",
        "  and long_answer['end_token'] != -1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d9yy_i61quM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_annotation_dataset(document_text, annotations):\n",
        "  orig_long_answer = annotations['long_answer']\n",
        "  new_long_answer = ' '.join(document_text[\n",
        "                                       orig_long_answer['start_token']:\n",
        "                                       orig_long_answer['end_token']\n",
        "                                      ])\n",
        "  orig_short_answer = annotations['short_answers']\n",
        "  new_short_answer = ' '.join(document_text[\n",
        "                                            orig_short_answer[0]['start_token']:\n",
        "                                            orig_short_answer[0]['end_token']\n",
        "                                          ]) if len(orig_short_answer) else ''\n",
        "  return {\n",
        "      \"yes_no_answer\": annotations[\"yes_no_answer\"],\n",
        "      \"long_answer\": new_long_answer,\n",
        "      \"short_answer\": new_short_answer\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QREZWH8MO26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write to simplified-nq-train.csv\n",
        "with open('simplified-nq-train.csv', 'w', newline='') as csvfile:\n",
        "  writer = csv.writer(csvfile, delimiter=';')\n",
        "  # headline\n",
        "  writer.writerow([\n",
        "                   EXAMPLE_ID,\n",
        "                   QUESTION_TEXT,\n",
        "                   LONG_ANSWER_CANDIDATES,\n",
        "                   ANNOTATIONS\n",
        "                   ])\n",
        "\n",
        "  with open('simplified-nq-train.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "      json_obj = json.loads(line)\n",
        "      if is_long_answer_existed(json_obj[ANNOTATIONS][0]):\n",
        "        document_text = json_obj[DOCUMENT_TEXT].split(' ')\n",
        "        writer.writerow([\n",
        "                        json_obj[EXAMPLE_ID], \n",
        "                        json_obj[QUESTION_TEXT],\n",
        "                        [\n",
        "                          ' '.join(document_text[candidate['start_token']:candidate['end_token']]) \n",
        "                          for candidate in json_obj[LONG_ANSWER_CANDIDATES]\n",
        "                        ],\n",
        "                        make_annotation_dataset(document_text, json_obj[ANNOTATIONS][0])\n",
        "                        ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svr9VMDsPwyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write to simplified-nq-test.csv\n",
        "with open('simplified-nq-test.csv', 'w', newline='') as csvfile:\n",
        "  writer = csv.writer(csvfile, delimiter=';')\n",
        "  # headline\n",
        "  writer.writerow([\n",
        "                   EXAMPLE_ID,\n",
        "                   QUESTION_TEXT, \n",
        "                   LONG_ANSWER_CANDIDATES\n",
        "                   ])\n",
        "\n",
        "  with open('simplified-nq-test.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "      json_obj = json.loads(line)\n",
        "      document_text = json_obj[DOCUMENT_TEXT].split(' ')\n",
        "\n",
        "      writer.writerow([\n",
        "                       json_obj[EXAMPLE_ID], \n",
        "                       json_obj[QUESTION_TEXT],\n",
        "                       [\n",
        "                        ' '.join(document_text[candidate['start_token']:candidate['end_token']]) \n",
        "                        for candidate in json_obj[LONG_ANSWER_CANDIDATES]\n",
        "                       ],\n",
        "                      ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hugm22sMVtOD",
        "colab_type": "text"
      },
      "source": [
        "### Move these generated csv files to my google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gTvejGEHGIF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbd43bab-3dd6-4036-acd8-bd730a2fdb0b"
      },
      "source": [
        "!mv simplified-nq-test.csv drive/My\\ Drive/\n",
        "!mv simplified-nq-train.csv drive/My\\ Drive/"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'simplified-nq-test.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CKOinmsWByl",
        "colab_type": "code",
        "outputId": "b42b248e-b25a-4e3e-8208-9e5480ae4f82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "# load training data and check if it's ok\n",
        "simplified_train_csv_path = \"drive/My Drive/simplified-nq-train.csv\"\n",
        "\n",
        "simplified_train_pd = pd.read_csv(simplified_train_csv_path, sep=';', chunksize=1)\n",
        "next(simplified_train_pd)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>question_text</th>\n",
              "      <th>long_answer_candidates</th>\n",
              "      <th>annotations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5655493461695504401</td>\n",
              "      <td>which is the most common use of opt-in e-mail ...</td>\n",
              "      <td>['&lt;Table&gt; &lt;Tr&gt; &lt;Td&gt; &lt;/Td&gt; &lt;Td&gt; ( hide ) This a...</td>\n",
              "      <td>{'yes_no_answer': 'NONE', 'long_answer': {'sta...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            example_id  ...                                        annotations\n",
              "0  5655493461695504401  ...  {'yes_no_answer': 'NONE', 'long_answer': {'sta...\n",
              "\n",
              "[1 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeolX8NVPcar",
        "colab_type": "code",
        "outputId": "89d63540-8554-4f58-931c-0976365728ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load testing data and check if it's ok\n",
        "simplified_test_csv_path = \"drive/My Drive/simplified-nq-test.csv\"\n",
        "\n",
        "simplified_test_pd = pd.read_csv(simplified_test_csv_path, sep=';')\n",
        "simplified_test_pd.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>question_text</th>\n",
              "      <th>long_answer_candidates</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1220107454853145579</td>\n",
              "      <td>who is the south african high commissioner in ...</td>\n",
              "      <td>['&lt;Table&gt; &lt;Tr&gt; &lt;Th_colspan=\"2\"&gt; High Commissio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8777415633185303067</td>\n",
              "      <td>the office episode when they sing to michael</td>\n",
              "      <td>['&lt;Table&gt; &lt;Tr&gt; &lt;Th_colspan=\"2\"&gt; `` Michael \\'s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4640548859154538040</td>\n",
              "      <td>what is the main idea of the cross of gold speech</td>\n",
              "      <td>['&lt;Table&gt; Cross of Gold speech &lt;Tr&gt; &lt;Td_colspa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-5316095317154496261</td>\n",
              "      <td>when was i want to sing in opera written</td>\n",
              "      <td>['&lt;Table&gt; &lt;Tr&gt; &lt;Th_colspan=\"2\"&gt; Wilkie Bard &lt;/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-8752372642178983917</td>\n",
              "      <td>who does the voices in ice age collision course</td>\n",
              "      <td>['&lt;Table&gt; &lt;Tr&gt; &lt;Th_colspan=\"2\"&gt; Ice Age : Coll...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            example_id  ...                             long_answer_candidates\n",
              "0 -1220107454853145579  ...  ['<Table> <Tr> <Th_colspan=\"2\"> High Commissio...\n",
              "1  8777415633185303067  ...  ['<Table> <Tr> <Th_colspan=\"2\"> `` Michael \\'s...\n",
              "2  4640548859154538040  ...  ['<Table> Cross of Gold speech <Tr> <Td_colspa...\n",
              "3 -5316095317154496261  ...  ['<Table> <Tr> <Th_colspan=\"2\"> Wilkie Bard </...\n",
              "4 -8752372642178983917  ...  ['<Table> <Tr> <Th_colspan=\"2\"> Ice Age : Coll...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUvno3lZYcSp",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Short Answer Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1AFfkJX5yvS",
        "colab_type": "text"
      },
      "source": [
        "For long answers that exist, there are several cases of short answers:\n",
        "1. YES/NO\n",
        "2. a sentence or phrase\n",
        "3. no short answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PYQ3NcYbNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Short_answer_dataset():\n",
        "  def __init__(self, tokenizer, data_path='drive/My Drive/simplified-nq-train.csv', mode='train'):\n",
        "    assert mode in ['train', 'test']\n",
        "    self.df = pd.read_csv(data_path, sep=\";\", chunksize=1)\n",
        "    self.mode = mode\n",
        "    self.tokenizer = tokenizer\n",
        "    self.yes_no_label_map = {'YES': 1, 'NO': 2, 'NONE': 3}\n",
        "    self._next_index = 0\n",
        "\n",
        "\n",
        "  def get_dataset_generator_function(self):\n",
        "    return self._create_data_generator\n",
        "\n",
        "\n",
        "  '''\n",
        "  target_format: 'raw'|'bert'\n",
        "  '''\n",
        "  def _create_data_generator(self, target_format='bert'):\n",
        "    temp_next_index = 0\n",
        "    for instance in self.df:\n",
        "      if temp_next_index == self._next_index:\n",
        "        self._next_index += 1\n",
        "        if target_format == 'bert':\n",
        "          yield self.get_bert_compatible_instance(instance)\n",
        "        else:\n",
        "          yield instance\n",
        "      else:\n",
        "        temp_next_index += 1\n",
        "\n",
        "\n",
        "  '''\n",
        "  make a pair that consists of question text and long answer, then return 3 tensors\n",
        "  for the pair:\n",
        "  - tokens_tensor：tokens list made from concatenating two sentences. special tokens are included([CLS], [SEP], etc.)\n",
        "  - segments_tensor： classify the boundary of each sentence; 0 for the first sentence, 1 for the second sentence\n",
        "  - masks_tensor\n",
        "  - label_tensor： none if it's in testing mode\n",
        "  '''\n",
        "  def get_bert_compatible_instance(self, instance):\n",
        "    '''\n",
        "    helper functions\n",
        "    '''\n",
        "    def _get_question_long_answer_pair(instance):\n",
        "      question_text = instance[\"question_text\"].iloc[0]\n",
        "      json_obj = ast.literal_eval(instance[\"annotations\"].iloc[0])\n",
        "      long_answer_text = json_obj[\"long_answer\"]\n",
        "      return question_text, long_answer_text\n",
        "\n",
        "    def _get_short_answer_label(instance):\n",
        "      json_obj = ast.literal_eval(instance[\"annotations\"].iloc[0])\n",
        "      return json_obj[\"yes_no_answer\"]\n",
        "\n",
        "    ### build label_tensor\n",
        "    if self.mode == 'train':\n",
        "      short_answer_label = _get_short_answer_label(instance)\n",
        "      label_id = self.yes_no_label_map[short_answer_label]\n",
        "      label_tensor = tf.constant(label_id, dtype=tf.int64)\n",
        "    else:\n",
        "      label_tensor = None\n",
        "\n",
        "    # question_text is the first sentence(a)\n",
        "    # long_answer_text is the second sentence(b)\n",
        "    question_text, long_answer_text = _get_question_long_answer_pair(instance)\n",
        "    \n",
        "    ### build tokens_tensor\n",
        "    # first sentence\n",
        "    word_pieces = [\"[CLS]\"]\n",
        "    tokens_a = self.tokenizer.tokenize(question_text)\n",
        "    word_pieces += tokens_a + [\"[SEP]\"]\n",
        "    len_a = len(word_pieces)\n",
        "\n",
        "    # second sentence\n",
        "    tokens_b = self.tokenizer.tokenize(long_answer_text)\n",
        "    word_pieces += tokens_b + [\"[SEP]\"]\n",
        "    len_b = len(word_pieces) - len_a\n",
        "\n",
        "    ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "    tokens_tensor = tf.constant(ids, dtype=tf.int64)\n",
        "\n",
        "    ### build segments_tensor\n",
        "    segments_tensor = tf.constant([0] * len_a + [1] * len_b, dtype=tf.int64)\n",
        "\n",
        "    ### build masks_tensor\n",
        "    masks_tensors = tf.zeros(tokens_tensor.shape, dtype=tf.int64)\n",
        "    masks_tensors = tf.where(tokens_tensor != 0 , 1, 0)\n",
        "\n",
        "    return (tokens_tensor, segments_tensor, masks_tensors), label_tensor\n",
        "\n",
        "  \n",
        "  def convert_ids_to_tokens(self, tokens_tensor):\n",
        "    return self.tokenizer.convert_ids_to_tokens(tokens_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clRtbCkozf0_",
        "colab_type": "text"
      },
      "source": [
        "### Initialize BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qBpoxcnzZWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_basic_tokenize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzeH5voph-z9",
        "colab_type": "text"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8dhWJhjh94J",
        "colab_type": "code",
        "outputId": "42f51e7d-f45e-4aa9-9f77-51228dd38b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "short_answer_train_dataset_gen = Short_answer_dataset(bert_tokenizer).get_dataset_generator_function()\n",
        "short_answer_train_dataset = tf.data.Dataset.from_generator(\n",
        "    short_answer_train_dataset_gen, \n",
        "    output_types=((tf.int64, tf.int64, tf.int64), tf.int64)\n",
        ")\n",
        "\n",
        "for instance in short_answer_train_dataset:\n",
        "  print(instance)\n",
        "  break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((<tf.Tensor: shape=(92,), dtype=int64, numpy=\n",
            "array([  101,  1134,  1110,  1103,  1211,  1887,  1329,  1104, 11769,\n",
            "        1204, 28137,  1394,   174, 28137, 14746,  6213,   102,   133,\n",
            "        2101, 28144,   138,  1887,  1859,  1104,  6156,  6213,  1110,\n",
            "         170, 24343,  1850,  1106,  1126,  6437,  3016,   112,  1116,\n",
            "        5793,   119,  5723, 24343,  1116, 12862,  5793,  1104,  8851,\n",
            "        1958,  1137, 18949,   117,  1137,  1207,  2982,   119,  1130,\n",
            "        1142,  2076,  1104,  6437,   117,   170,  1419,  1115,  3349,\n",
            "        1106,  3952,   170, 24343,  1106,  1147,  5793,  1336,  2367,\n",
            "        1172,  1120,  1103,  1553,  1104,  4779,  1191,  1152,  1156,\n",
            "        1176,  1106,  3531,  1103, 24343,   119,   133, 28139,  2101,\n",
            "       28144,   102])>, <tf.Tensor: shape=(92,), dtype=int64, numpy=\n",
            "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1])>, <tf.Tensor: shape=(92,), dtype=int64, numpy=\n",
            "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1])>), <tf.Tensor: shape=(), dtype=int64, numpy=3>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIAtoE_4lF8D",
        "colab_type": "text"
      },
      "source": [
        "# Short Answer Idenfiticator\n",
        "\n",
        "![short-answer-identificator](https://github.com/cyyeh/kaggle/blob/master/google-qa/short_answer_identificator.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejwRXuwwn27g",
        "colab_type": "text"
      },
      "source": [
        "## Long Answer Encoder\n",
        "\n",
        "see Prepare Short Answer Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvPQV4fkn5Rm",
        "colab_type": "text"
      },
      "source": [
        "## Short Answer Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ9uPnnM69h0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_VazWBfn-zy",
        "colab_type": "text"
      },
      "source": [
        "## Short Answer Null Classifier"
      ]
    }
  ]
}