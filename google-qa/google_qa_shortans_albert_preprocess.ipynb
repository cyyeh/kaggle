{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "google-qa-shortans-albert-preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyyeh/kaggle/blob/master/google-qa/google_qa_shortans_albert_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKOHe0p-Ht5R",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjXVfwTCbcDw",
        "colab_type": "code",
        "outputId": "494292de-764e-4fb3-ecbe-1e0ba6a66a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make sure colab use tf2.x\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onr7kcYyH0Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWrB_OZ5Hyvw",
        "colab_type": "code",
        "outputId": "4360d6af-11c4-494b-d701-d7442d75a23e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install transformers # BertModel"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /tensorflow-2.1.0/python3.6 (from transformers) (1.18.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 59.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 34.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 55.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: requests in /tensorflow-2.1.0/python3.6 (from transformers) (2.22.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from sacremoses->transformers) (1.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (1.25.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=44b88890743dd662bacd81da13a71160c541acae39b31321aaf42b7c52241c50\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvwA3dckYrvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AlbertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSYR9qivH3cE",
        "colab_type": "text"
      },
      "source": [
        "# Prepare YES/NO Answer Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGlXk3heH5um",
        "colab_type": "code",
        "outputId": "e16821fa-0394-412a-ba71-7e39b6177d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E4qELtHIHGL",
        "colab_type": "text"
      },
      "source": [
        "Check if training/testing dataset is available in your google drive. If it's not available, you should run code inside the \"Prepare Kaggle Dataset\" section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8_Ex5YQIDNo",
        "colab_type": "code",
        "outputId": "ab53fb38-c8a8-4b8a-96f7-7d81b1c0637d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if os.path.exists('drive/My Drive/yes_no_ans_df.pkl') and \\\n",
        "os.path.exists('drive/My Drive/short_ans_raw_df.pkl'):\n",
        "  print(\"Training dataset is available!\")\n",
        "else:\n",
        "  print(\"Training dataset is not found, please run code inside the 'Prepare Kaggle Dataset' section.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset is not found, please run code inside the 'Prepare Kaggle Dataset' section.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciW0IPzaIO1I",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Kaggle Dataset for [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QowWvieAIRDR",
        "colab_type": "text"
      },
      "source": [
        "### Download Question Answering Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUOkqK_tISun",
        "colab_type": "code",
        "outputId": "ff51c136-51dc-4ee1-b8cc-9177f73d1f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        }
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"chihyuyeh\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"f21b340fc8082977cbf954c80ad69ae1\" # key from the json file\n",
        "!kaggle competitions download -c tensorflow2-question-answering"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading simplified-nq-train.jsonl.zip to /content\n",
            " 86% 3.84G/4.46G [00:48<00:13, 50.6MB/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHZnsqknn99x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip simplified-nq-train.jsonl.zip\n",
        "!unzip simplified-nq-test.jsonl.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hv1uHLsBPfs",
        "colab_type": "text"
      },
      "source": [
        "### Generate Short Answer Raw Data Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ9gMqFtIbby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = 'simplified-nq-train.jsonl'\n",
        "test_path = 'simplified-nq-test.jsonl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBDNb--Tn49q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extracting_text_using_start_end_token_id(document_text, start_token, end_token):\n",
        "    splitted_document_text = document_text.split()\n",
        "    return ' '.join(splitted_document_text[start_token:end_token])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH50Z7gin7-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def has_long_answer(long_answer_candidate):\n",
        "  return long_answer_candidate['start_token'] != -1 \\\n",
        "  and long_answer_candidate['candidate_index'] != -1 \\\n",
        "  and long_answer_candidate['end_token'] != -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaXuSGr7BL9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_cleaning_for_short_answer(\n",
        "  json_obj,\n",
        "  task='both',\n",
        "  example_id=True):\n",
        "  '''12\n",
        "  keys of the output dictionary: \n",
        "    'example_id' # optional \n",
        "    'question_text'\n",
        "    'long_answer_text'\n",
        "    'yes_no_answer' # exist only if task == 'both' or 'classing'\n",
        "    'short_answer_start_token' # exist only  if task == 'both' or 'squading'\n",
        "    'short_answer_end_token' # exist only if task == 'both' or 'squading'\n",
        "  ''' \n",
        "  assert task == 'classing' or task == 'squading' or task == 'both'\n",
        "  new_data_d = {}\n",
        "  # assignment for both tasks  \n",
        "  annotations = json_obj['annotations'][0]\n",
        "  long_answer_candidate = annotations['long_answer']\n",
        "  if example_id:\n",
        "    new_data_d['example_id'] = json_obj['example_id']\n",
        "  new_data_d['question_text'] = json_obj['question_text']\n",
        "  long_ans_start = long_answer_candidate['start_token']\n",
        "  long_ans_end = long_answer_candidate['end_token']\n",
        "  new_data_d['long_answer_text'] = (\n",
        "    extracting_text_using_start_end_token_id(\n",
        "      json_obj['document_text'],\n",
        "      long_ans_start,\n",
        "      long_ans_end\n",
        "    ))\n",
        "  if task != 'both':\n",
        "    if task == 'squading':\n",
        "      short_answer_candidate = annotations['short_answers']\n",
        "      if not short_answer_candidate:\n",
        "        short_ans_start = -1\n",
        "        short_ans_end = -1\n",
        "      else:\n",
        "        short_ans_start = short_answer_candidate[0]['start_token'] - long_ans_start\n",
        "        short_ans_end = short_answer_candidate[0]['end_token'] - long_ans_start\n",
        "      new_data_d['short_answer_start_token'] = short_ans_start\n",
        "      new_data_d['short_answer_end_token'] = short_ans_end\n",
        "    elif task == 'classing':\n",
        "      new_data_d['yes_no_answer'] = annotations['yes_no_answer']\n",
        "  else:\n",
        "    # get squading labels \n",
        "    short_answer_candidate = annotations['short_answers']\n",
        "    if not short_answer_candidate:\n",
        "      short_ans_start = -1\n",
        "      short_ans_end = -1\n",
        "    else:\n",
        "      short_ans_start = short_answer_candidate[0]['start_token'] - long_ans_start\n",
        "      short_ans_end = short_answer_candidate[0]['end_token'] - long_ans_start\n",
        "    new_data_d['short_answer_start_token'] = short_ans_start\n",
        "    new_data_d['short_answer_end_token'] = short_ans_end\n",
        "    # get classing labels \n",
        "    new_data_d['yes_no_answer'] = annotations['yes_no_answer']\n",
        "  return new_data_d "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_msdHUPaR58R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_short_answer_dataset(path):\n",
        "  short_answer_dataset = []\n",
        "  with open(path) as f:\n",
        "    for line in f:\n",
        "      old_data_d = json.loads(line)\n",
        "      if has_long_answer(old_data_d['annotations'][0]['long_answer']):\n",
        "        new_data_d = data_cleaning_for_short_answer(old_data_d)\n",
        "        short_answer_dataset.append(new_data_d)\n",
        "  return pd.DataFrame(short_answer_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTNxPQ49sVoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_df = create_short_answer_dataset(train_path)\n",
        "\n",
        "print(len(raw_df))\n",
        "print(raw_df.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNl7RdMIKQIJ",
        "colab_type": "text"
      },
      "source": [
        "### Save Short Answer Dataset to Pickle Format and Export It To Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D3Y9eulKPeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_df.to_pickle(\"./short_ans_raw_df.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5SzPpxgKkQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ./short_ans_raw_df.pkl /content/drive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O4zpkqVBe6-",
        "colab_type": "text"
      },
      "source": [
        "### Create YES/NO Answer Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldxQqUgGLduv",
        "colab_type": "code",
        "outputId": "3a11386b-c6f0-4e41-8bf6-c4b9c7343025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "raw_df = pd.read_pickle(\"drive/My Drive/short_ans_raw_df.pkl\")\n",
        "raw_df = raw_df[:5000]\n",
        "raw_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>question_text</th>\n",
              "      <th>long_answer_text</th>\n",
              "      <th>yes_no_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5655493461695504401</td>\n",
              "      <td>which is the most common use of opt-in e-mail ...</td>\n",
              "      <td>&lt;P&gt; A common example of permission marketing i...</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5328212470870865242</td>\n",
              "      <td>how i.met your mother who is the mother</td>\n",
              "      <td>&lt;P&gt; Tracy McConnell , better known as `` The M...</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4435104480114867852</td>\n",
              "      <td>what type of fertilisation takes place in humans</td>\n",
              "      <td>&lt;P&gt; The process of fertilization involves a sp...</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5289242154789678439</td>\n",
              "      <td>who had the most wins in the nfl</td>\n",
              "      <td>&lt;P&gt; Active quarterback Tom Brady holds the rec...</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2500044561429484630</td>\n",
              "      <td>who played mantis guardians of the galaxy 2</td>\n",
              "      <td>&lt;P&gt; Pom Klementieff ( born 3 May 1986 ) is a F...</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            example_id  ... yes_no_answer\n",
              "0  5655493461695504401  ...          NONE\n",
              "1  5328212470870865242  ...          NONE\n",
              "2  4435104480114867852  ...          NONE\n",
              "3  5289242154789678439  ...          NONE\n",
              "4 -2500044561429484630  ...          NONE\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV7qyM0F2OjH",
        "colab_type": "text"
      },
      "source": [
        "### Original Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2tJ0TMaa1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_question_tokens(tokenizer, question_text):\n",
        "  question_tokens = ['[CLS]'] + tokenizer.tokenize(question_text) + ['[SEP]']\n",
        "  return question_tokens\n",
        "\n",
        "\n",
        "def get_long_answer_tokens(tokenizer, question_tokens,long_answer_text):\n",
        "  tokens = question_tokens + tokenizer.tokenize(long_answer_text)\\\n",
        "    + ['[SEP]']\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def get_long_answer_tokens_and_start_end_tokens(\n",
        "  tokenizer, \n",
        "  question_tokens,\n",
        "  long_answer_text,\n",
        "  short_answer_start_token, \n",
        "  short_answer_end_token\n",
        "  ):\n",
        "  long_answer_tokens = long_answer_text.split()\n",
        "  chunk_1 = ' '.join(long_answer_tokens[:short_answer_start_token])\n",
        "  chunk_2 = ' '.join(long_answer_tokens[short_answer_start_token:short_answer_end_token])\n",
        "  chunk_3 = ' '.join(long_answer_tokens[short_answer_end_token:])\n",
        "  # handle new start end token\n",
        "  tokens = question_tokens + tokenizer.tokenize(chunk_1)\n",
        "  label_start_token = len(tokens)\n",
        "  tokens = tokens + tokenizer.tokenize(chunk_2)\n",
        "  label_end_token = len(tokens)\n",
        "  tokens = tokens + tokenizer.tokenize(chunk_3) + ['[SEP]']\n",
        "  return tokens, label_start_token, label_end_token\n",
        "\n",
        "\n",
        "def generate_short_ans_feature(row, task, MAX_LENGTH = 512):\n",
        "  # This function takes a row of the short answer dataframe as input\n",
        "  # and outputs a dict with the following \n",
        "  # keys: \n",
        "  # 1. token_ids\n",
        "  # 2. segment_ids \n",
        "  # 3. mask_ids \n",
        "  # 4. label_yes_no (if task == classing or both)  \n",
        "  # 5. label_start/end_token (if task == classing or squading) \n",
        "  label_yes_no_map = {\n",
        "    'YES': 0,\n",
        "    'NO': 1,\n",
        "    'NONE': 2\n",
        "  }\n",
        "  short_ans_feature_dict = {}\n",
        "  question_text = row.question_text\n",
        "  long_answer_text = row.long_answer_text\n",
        "  if task == 'squading':\n",
        "    short_answer_start_token = row.short_answer_start_token\n",
        "    short_answer_end_token = row.short_answer_end_token\n",
        "  elif task == 'classing': \n",
        "    short_ans_feature_dict['label_yes_no'] = label_yes_no_map[row.yes_no_answer]\n",
        "  else:\n",
        "    short_answer_start_token = row.short_answer_start_token\n",
        "    short_answer_end_token = row.short_answer_end_token\n",
        "    short_ans_feature_dict['label_yes_no'] = label_yes_no_map[row.yes_no_answer]\n",
        "  question_tokens = get_question_tokens(\n",
        "    tokenizer,\n",
        "    row.question_text)\n",
        "  sentence_A_len = len(question_tokens)\n",
        "  # tokenize long answer if no short answer entities exists \n",
        "  if task == 'classing' or short_answer_start_token == -1:\n",
        "    tokens = get_long_answer_tokens(\n",
        "      tokenizer, \n",
        "      question_tokens,\n",
        "      long_answer_text)\n",
        "    sentence_len = len(tokens)\n",
        "    label_start_token = 0\n",
        "    label_end_token = 0\n",
        "  # tokenize short answer span\n",
        "  else:\n",
        "    # cut long answer into 3 chunks\n",
        "    tokens, label_start_token, label_end_token = get_long_answer_tokens_and_start_end_tokens(\n",
        "      tokenizer, \n",
        "      question_tokens,\n",
        "      long_answer_text,\n",
        "      short_answer_start_token, \n",
        "      short_answer_end_token\n",
        "      )\n",
        "    sentence_len = len(tokens)\n",
        "  # apply truncating\n",
        "  if sentence_len > MAX_LENGTH:\n",
        "    tokens = tokens[:MAX_LENGTH-1] + ['[SEP]']\n",
        "    sentence_len = MAX_LENGTH\n",
        "  if label_end_token > MAX_LENGTH - 1: # should not exceed last token [SEP]\n",
        "    label_start_token = 0\n",
        "    label_end_token = 0\n",
        "  # create segment_id and mask_id\n",
        "  segment_ids = sentence_A_len * [0] + (sentence_len - sentence_A_len) * [1] \n",
        "  mask_ids = sentence_len * [1]\n",
        "  # apply padding\n",
        "  if (sentence_len < MAX_LENGTH):\n",
        "    pad_len = MAX_LENGTH - sentence_len\n",
        "    tokens = tokens + pad_len * ['[PAD]']\n",
        "    segment_ids = segment_ids + pad_len * [0]\n",
        "    mask_ids = mask_ids + pad_len * [0]\n",
        "  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  # append to lists\n",
        "  short_ans_feature_dict['token_ids'] = token_ids\n",
        "  short_ans_feature_dict['segment_ids'] = segment_ids\n",
        "  short_ans_feature_dict['mask_ids'] = mask_ids\n",
        "  if task != 'classing':\n",
        "    short_ans_feature_dict['label_start_tokens'] = label_start_token\n",
        "    short_ans_feature_dict['label_end_tokens'] = label_end_token\n",
        "  return short_ans_feature_dict\n",
        "\n",
        "\n",
        "def create_short_ans_features(\n",
        "  raw_df, \n",
        "  tokenizer, \n",
        "  task='classing'):\n",
        "  '''\n",
        "  parameters:\n",
        "  raw_df: short answer dataframe\n",
        "  task: 'classing' (default): yes/no answer; 'squading': short answer entity\n",
        "  returns:\n",
        "  dataframe of tokenized short answer dataset\n",
        "  '''\n",
        "  # assertions \n",
        "  assert task == 'classing' or task == 'squading' or task == 'both'\n",
        "  if task == 'classing':\n",
        "    assert \"yes_no_answer\" in raw_df.columns \n",
        "  elif task == 'squading':\n",
        "    assert \"short_answer_start_token\" in raw_df.columns \n",
        "    assert \"short_answer_end_token\" in raw_df.columns \n",
        "  else:\n",
        "    assert \"yes_no_answer\" in raw_df.columns \n",
        "    assert \"short_answer_start_token\" in raw_df.columns \n",
        "    assert \"short_answer_end_token\" in raw_df.columns\n",
        "\n",
        "  dict_list = [generate_short_ans_feature(row,task) for _, row in raw_df.iterrows()]\n",
        "  return pd.DataFrame(dict_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bysNgMUYa5KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B3LnXYLpzlr",
        "colab_type": "code",
        "outputId": "c94c3142-7988-48b1-e181-f3f5142c6a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "yes_no_ans_df = create_short_ans_features(raw_df, tokenizer, task='squading')\n",
        "yes_no_ans_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 36.6 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zXhw92-AyNGU"
      },
      "source": [
        "### Save YES/NO Answer Dataset to Pickle Format and Export It To Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj1gSOK06W1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save dataframe to pickle file\n",
        "yes_no_ans_df.to_pickle(\"./yes_no_ans_df.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ob60ZPqk23w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ./yes_no_ans_df.pkl /content/drive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncl-lcPMNQfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}