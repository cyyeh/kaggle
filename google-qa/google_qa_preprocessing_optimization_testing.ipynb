{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "google-qa-preprocessing-optimization-testing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BZmdIjKGbLsE"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNhAcQK//XtvdhSi446rzDW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyyeh/kaggle/blob/master/google-qa/google_qa_preprocessing_optimization_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBNM5z5L1kBt",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing Optimization Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEQthE1B2bCe",
        "colab_type": "text"
      },
      "source": [
        "### TODOS\n",
        "\n",
        "- [ ] Optimization for reading a large file(16.26GB)\n",
        "- [ ] Optimization for computation in dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yAPrL_A1yPZ",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGEmH55Y11mN",
        "colab_type": "code",
        "outputId": "134fe66d-f528-40a2-f293-7d2524e11fea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make sure colab use tf2.x\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlMyl2-W13HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7DoDiRa14kz",
        "colab_type": "code",
        "outputId": "d612fd98-aa28-4f0b-db6f-3757c5b4dc9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install transformers # BertModel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/58/3d789b98923da6485f376be1e04d59ad7003a63bdb2b04b5eea7e02857e5/transformers-2.5.0-py3-none-any.whl (481kB)\n",
            "\r\u001b[K     |▊                               | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 491kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /tensorflow-2.1.0/python3.6 (from transformers) (2.22.0)\n",
            "Requirement already satisfied: numpy in /tensorflow-2.1.0/python3.6 (from transformers) (1.18.1)\n",
            "Collecting tokenizers==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/1d/ea7e2c628942e686595736f73678348272120d026b7acd54fe43e5211bb1/tokenizers-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 33.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (1.25.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from sacremoses->transformers) (1.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=6e37558f3ffb41a925f96bbe5fbd48a1341867bc6e727ccf5a1ad497b7ffa873\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.0 transformers-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyNljpzs185W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AlbertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arf3E8x3dURt",
        "colab_type": "text"
      },
      "source": [
        "### Import `Preprocessor.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URnBPIYudeYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Preprocessor import has_long_answer, data_cleaning_for_short_answer\n",
        "\n",
        "path = 'simplified-nq-train.jsonl.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNg0vQRY1onB",
        "colab_type": "text"
      },
      "source": [
        "## Download Training Dataset(16.26GB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWrMfZWi1Z65",
        "colab_type": "code",
        "outputId": "3d4e151b-0dfd-4722-8bf8-777f076c39fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if not os.path.exists(path):\n",
        "  import os\n",
        "  os.environ['KAGGLE_USERNAME'] = \"chihyuyeh\" # username from the json file\n",
        "  os.environ['KAGGLE_KEY'] = \"f21b340fc8082977cbf954c80ad69ae1\" # key from the json file\n",
        "  !kaggle competitions download -c tensorflow2-question-answering -f simplified-nq-train.jsonl"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading simplified-nq-train.jsonl.zip to /content\n",
            "100% 4.46G/4.46G [00:59<00:00, 46.3MB/s]\n",
            "100% 4.46G/4.46G [00:59<00:00, 80.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX8QujKX23ms",
        "colab_type": "text"
      },
      "source": [
        "## Optimization for reading a large file(16.26GB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZmdIjKGbLsE",
        "colab_type": "text"
      },
      "source": [
        "### Method 1: using `multiprocessing`\n",
        "\n",
        "issues\n",
        "- code is dirty and hard to read\n",
        "- still not sure if GIL limits the performance here\n",
        "- how to combine sub-results to one big end result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A7wGC672DUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing as mp\n",
        "import os\n",
        "import dill\n",
        "\n",
        "short_answer_dataset = []\n",
        "\n",
        "def process_data(line):\n",
        "  json_obj = json.loads(line)\n",
        "  if has_long_answer(json_obj['annotations'][0]['long_answer']):\n",
        "    short_answer_dataset.append(data_cleaning_for_short_answer(json_obj))\n",
        "  print(len(short_answer_dataset))\n",
        "\n",
        "def process_wrapper(chunk_start, chunk_size):\n",
        "  with open(path) as f:\n",
        "    f.seek(chunk_start)\n",
        "    lines = f.read(chunk_size).splitlines()\n",
        "    for line in lines:\n",
        "      process_data(line)\n",
        "\n",
        "def chunkify(path, size=1024*1024):\n",
        "  file_end = os.path.getsize(path)\n",
        "  with open(path, 'rb') as f:\n",
        "    chunk_end = f.tell()\n",
        "    while True:\n",
        "      chunk_start = chunk_end\n",
        "      f.seek(size, 1)\n",
        "      f.readline()\n",
        "      chunk_end = f.tell()\n",
        "      yield chunk_start, chunk_end - chunk_start\n",
        "      if chunk_end > file_end:\n",
        "        break\n",
        "\n",
        "def run_dill_encoded(payload):\n",
        "  fun, args = dill.loads(payload)\n",
        "  return fun(*args)\n",
        "\n",
        "def apply_async(pool, fun, args):\n",
        "  payload = dill.dumps((fun, args))\n",
        "  return pool.apply_async(run_dill_encoded, (payload,))\n",
        "\n",
        "# init objects\n",
        "pool = mp.Pool()\n",
        "jobs = []\n",
        "\n",
        "# create jobs\n",
        "for chunk_start, chunk_size in chunkify(path):\n",
        "  job = apply_async(pool, process_wrapper, (chunk_start, chunk_size))\n",
        "  jobs.append(job)\n",
        "\n",
        "# wait for all jobs to finish\n",
        "for job in jobs:\n",
        "  job.get()\n",
        "\n",
        "# clean up\n",
        "pool.close()\n",
        "\n",
        "raw_df = pd.DataFrame(short_answer_dataset)\n",
        "#print(len(raw_df))\n",
        "#print(raw_df.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijBmmj2Mg3Sl",
        "colab_type": "text"
      },
      "source": [
        "### Method 2: using Dask\n",
        "\n",
        "- bypass GIL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERHv_QJfv_fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dask.bag as db\n",
        "import json\n",
        "import pprint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP5TH4_8d3E6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clean_short_ans_df(path, task='both', example_id=False):\n",
        "  return (db.read_text(path)\n",
        "           .map(json.loads)\n",
        "           .filter(lambda json_obj: has_long_answer(\n",
        "             json_obj['annotations'][0]['long_answer']\n",
        "           ))\n",
        "           .map(lambda json_obj: data_cleaning_for_short_answer(\n",
        "            json_obj,\n",
        "            task,\n",
        "            example_id   \n",
        "           ))\n",
        "           .to_dataframe()\n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGIYGqWyhHvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "14f19090-c301-456b-c21a-81180f105e9d"
      },
      "source": [
        "%%timeit -n1\n",
        "raw_df = get_short_ans_clean_df(path)\n",
        "\n",
        "print(len(raw_df))\n",
        "print(raw_df.columns)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152148\n",
            "Index(['question_text', 'long_answer_text', 'short_answer_start_token',\n",
            "       'short_answer_end_token', 'yes_no_answer'],\n",
            "      dtype='object')\n",
            "152148\n",
            "Index(['question_text', 'long_answer_text', 'short_answer_start_token',\n",
            "       'short_answer_end_token', 'yes_no_answer'],\n",
            "      dtype='object')\n",
            "152148\n",
            "Index(['question_text', 'long_answer_text', 'short_answer_start_token',\n",
            "       'short_answer_end_token', 'yes_no_answer'],\n",
            "      dtype='object')\n",
            "1 loop, best of 3: 5min 21s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7j2aHzIGICy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = (db.read_text(path)\n",
        "          .map(json.loads)\n",
        "          .filter(lambda json_obj: has_long_answer(\n",
        "            json_obj['annotations'][0]['long_answer']\n",
        "          ))\n",
        "          .map(lambda json_obj: data_cleaning_for_short_answer(\n",
        "            json_obj,\n",
        "            'both',\n",
        "            False   \n",
        "          ))\n",
        "          .take(100)\n",
        "        )\n",
        "\n",
        "for instance in data:\n",
        "  pprint.pprint(instance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thVziJZ6sBNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Preprocessor import get_clean_short_ans_df, get_clean_short_ans_tuple\n",
        "\n",
        "path = 'simplified-nq-train.jsonl.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQB0oUtJzXh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "613cc895-1b22-4d0e-ae8f-23c94f1933d9"
      },
      "source": [
        "%%timeit -n1\n",
        "raw_df = get_clean_short_ans_df(path)\n",
        "\n",
        "print(len(raw_df))\n",
        "print(raw_df.columns)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152148\n",
            "Index(['question_text', 'long_answer_text', 'short_answer_start_token',\n",
            "       'short_answer_end_token', 'yes_no_answer'],\n",
            "      dtype='object')\n",
            "152148\n",
            "Index(['question_text', 'long_answer_text', 'short_answer_start_token',\n",
            "       'short_answer_end_token', 'yes_no_answer'],\n",
            "      dtype='object')\n",
            "152148\n",
            "Index(['question_text', 'long_answer_text', 'short_answer_start_token',\n",
            "       'short_answer_end_token', 'yes_no_answer'],\n",
            "      dtype='object')\n",
            "1 loop, best of 3: 5min 3s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S35g2WLB0D2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e542500-18cf-492c-c7f8-bb7a690d5948"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "raw_data_tuple = get_clean_short_ans_tuple(path)\n",
        "\n",
        "for instance in raw_data_tuple:\n",
        "  pprint(instance)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'long_answer_text': '<P> A common example of permission marketing is a '\n",
            "                     \"newsletter sent to an advertising firm 's customers . \"\n",
            "                     'Such newsletters inform customers of upcoming events or '\n",
            "                     'promotions , or new products . In this type of '\n",
            "                     'advertising , a company that wants to send a newsletter '\n",
            "                     'to their customers may ask them at the point of purchase '\n",
            "                     'if they would like to receive the newsletter . </P>',\n",
            " 'question_text': 'which is the most common use of opt-in e-mail marketing',\n",
            " 'short_answer_end_token': 17,\n",
            " 'short_answer_start_token': 8,\n",
            " 'yes_no_answer': 'NONE'}\n",
            "{'long_answer_text': \"<P> Tracy McConnell , better known as `` The Mother '' , \"\n",
            "                     'is the title character from the CBS television sitcom '\n",
            "                     'How I Met Your Mother . The show , narrated by Future '\n",
            "                     'Ted , tells the story of how Ted Mosby met The Mother . '\n",
            "                     'Tracy McConnell appears in 8 episodes from `` Lucky '\n",
            "                     \"Penny '' to `` The Time Travelers '' as an unseen \"\n",
            "                     'character ; she was first seen fully in `` Something New '\n",
            "                     \"'' and was promoted to a main character in season 9 . \"\n",
            "                     'The Mother is played by Cristin Milioti . </P>',\n",
            " 'question_text': 'how i.met your mother who is the mother',\n",
            " 'short_answer_end_token': 3,\n",
            " 'short_answer_start_token': 1,\n",
            " 'yes_no_answer': 'NONE'}\n",
            "{'long_answer_text': '<P> The process of fertilization involves a sperm fusing '\n",
            "                     'with an ovum . The most common sequence begins with '\n",
            "                     'ejaculation during copulation , follows with ovulation , '\n",
            "                     'and finishes with fertilization . Various exceptions to '\n",
            "                     'this sequence are possible , including artificial '\n",
            "                     'insemination , in vitro fertilization , external '\n",
            "                     'ejaculation without copulation , or copulation shortly '\n",
            "                     'after ovulation . Upon encountering the secondary oocyte '\n",
            "                     ', the acrosome of the sperm produces enzymes which allow '\n",
            "                     'it to burrow through the outer jelly coat of the egg . '\n",
            "                     \"The sperm plasma then fuses with the egg 's plasma \"\n",
            "                     'membrane , the sperm head disconnects from its flagellum '\n",
            "                     'and the egg travels down the Fallopian tube to reach the '\n",
            "                     'uterus . </P>',\n",
            " 'question_text': 'what type of fertilisation takes place in humans',\n",
            " 'short_answer_end_token': -1,\n",
            " 'short_answer_start_token': -1,\n",
            " 'yes_no_answer': 'NONE'}\n",
            "{'long_answer_text': '<P> Active quarterback Tom Brady holds the records for '\n",
            "                     'most wins with 220 , most regular season wins with 195 , '\n",
            "                     'and most postseason wins with 25 , as of Week 16 of the '\n",
            "                     '2017 NFL season . Having played the entirety of his '\n",
            "                     \"career with the New England Patriots , each of Brady 's \"\n",
            "                     'win records also apply to wins with a single team . </P>',\n",
            " 'question_text': 'who had the most wins in the nfl',\n",
            " 'short_answer_end_token': 5,\n",
            " 'short_answer_start_token': 3,\n",
            " 'yes_no_answer': 'NONE'}\n",
            "{'long_answer_text': '<P> Pom Klementieff ( born 3 May 1986 ) is a French '\n",
            "                     'actress . She was trained at the Cours Florent drama '\n",
            "                     'school in Paris and has appeared in such films as Loup ( '\n",
            "                     \"2009 ) , Sleepless Night ( 2011 ) and Hacker 's Game ( \"\n",
            "                     '2015 ) . She plays the role of Mantis in the film '\n",
            "                     'Guardians of the Galaxy Vol. 2 ( 2017 ) and will appear '\n",
            "                     'in the same role in the film Avengers : Infinity War ( '\n",
            "                     '2018 ) . </P>',\n",
            " 'question_text': 'who played mantis guardians of the galaxy 2',\n",
            " 'short_answer_end_token': 3,\n",
            " 'short_answer_start_token': 1,\n",
            " 'yes_no_answer': 'NONE'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1JXohMS1Q3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}