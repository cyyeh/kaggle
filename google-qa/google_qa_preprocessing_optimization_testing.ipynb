{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "google-qa-preprocessing-optimization-testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMfYKnmPD0VgzNrvovLSP28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyyeh/kaggle/blob/master/google-qa/google_qa_preprocessing_optimization_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBNM5z5L1kBt",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing Optimization Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEQthE1B2bCe",
        "colab_type": "text"
      },
      "source": [
        "### TODOS\n",
        "\n",
        "- [ ] Optimization for reading a large file(16.26GB)\n",
        "- [ ] Optimization for computation in dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yAPrL_A1yPZ",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGEmH55Y11mN",
        "colab_type": "code",
        "outputId": "5ab195ae-91ee-4f27-9c91-d9d1b76a4638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make sure colab use tf2.x\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlMyl2-W13HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7DoDiRa14kz",
        "colab_type": "code",
        "outputId": "e5c405b4-74c2-4ab1-9f0a-7c8ecd69bf82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install transformers # BertModel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /tensorflow-2.1.0/python3.6 (from transformers) (1.18.1)\n",
            "Requirement already satisfied: tokenizers==0.5.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.0)\n",
            "Requirement already satisfied: requests in /tensorflow-2.1.0/python3.6 (from transformers) (2.22.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (1.25.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from sacremoses->transformers) (1.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyNljpzs185W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AlbertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arf3E8x3dURt",
        "colab_type": "text"
      },
      "source": [
        "### Import `Preprocessor.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URnBPIYudeYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Preprocessor import has_long_answer, data_cleaning_for_short_answer\n",
        "\n",
        "path = 'simplified-nq-train.jsonl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNg0vQRY1onB",
        "colab_type": "text"
      },
      "source": [
        "## Download Training Dataset(16.26GB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWrMfZWi1Z65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(path):\n",
        "  import os\n",
        "  os.environ['KAGGLE_USERNAME'] = \"chihyuyeh\" # username from the json file\n",
        "  os.environ['KAGGLE_KEY'] = \"f21b340fc8082977cbf954c80ad69ae1\" # key from the json file\n",
        "  !kaggle competitions download -c tensorflow2-question-answering -f simplified-nq-train.jsonl\n",
        "  !unzip simplified-nq-train.jsonl.zip\n",
        "  !rm simplified-nq-train.jsonl.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX8QujKX23ms",
        "colab_type": "text"
      },
      "source": [
        "## Optimization for reading a large file(16.26GB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZmdIjKGbLsE",
        "colab_type": "text"
      },
      "source": [
        "### Method 1: using `multiprocessing`\n",
        "\n",
        "issues\n",
        "- code is dirty and hard to read\n",
        "- still not sure if GIL limits the performance here\n",
        "- how to combine sub-results to one big end result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A7wGC672DUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing as mp\n",
        "import os\n",
        "import dill\n",
        "\n",
        "short_answer_dataset = []\n",
        "\n",
        "def process_data(line):\n",
        "  json_obj = json.loads(line)\n",
        "  if has_long_answer(json_obj['annotations'][0]['long_answer']):\n",
        "    short_answer_dataset.append(data_cleaning_for_short_answer(json_obj))\n",
        "  print(len(short_answer_dataset))\n",
        "\n",
        "def process_wrapper(chunk_start, chunk_size):\n",
        "  with open(path) as f:\n",
        "    f.seek(chunk_start)\n",
        "    lines = f.read(chunk_size).splitlines()\n",
        "    for line in lines:\n",
        "      process_data(line)\n",
        "\n",
        "def chunkify(path, size=1024*1024):\n",
        "  file_end = os.path.getsize(path)\n",
        "  with open(path, 'rb') as f:\n",
        "    chunk_end = f.tell()\n",
        "    while True:\n",
        "      chunk_start = chunk_end\n",
        "      f.seek(size, 1)\n",
        "      f.readline()\n",
        "      chunk_end = f.tell()\n",
        "      yield chunk_start, chunk_end - chunk_start\n",
        "      if chunk_end > file_end:\n",
        "        break\n",
        "\n",
        "def run_dill_encoded(payload):\n",
        "  fun, args = dill.loads(payload)\n",
        "  return fun(*args)\n",
        "\n",
        "def apply_async(pool, fun, args):\n",
        "  payload = dill.dumps((fun, args))\n",
        "  return pool.apply_async(run_dill_encoded, (payload,))\n",
        "\n",
        "# init objects\n",
        "pool = mp.Pool()\n",
        "jobs = []\n",
        "\n",
        "# create jobs\n",
        "for chunk_start, chunk_size in chunkify(path):\n",
        "  job = apply_async(pool, process_wrapper, (chunk_start, chunk_size))\n",
        "  jobs.append(job)\n",
        "\n",
        "# wait for all jobs to finish\n",
        "for job in jobs:\n",
        "  job.get()\n",
        "\n",
        "# clean up\n",
        "pool.close()\n",
        "\n",
        "raw_df = pd.DataFrame(short_answer_dataset)\n",
        "#print(len(raw_df))\n",
        "#print(raw_df.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijBmmj2Mg3Sl",
        "colab_type": "text"
      },
      "source": [
        "### Method 2: using Dask\n",
        "\n",
        "- bypass GIL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERHv_QJfv_fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dask.bag as db\n",
        "import json\n",
        "import pprint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP5TH4_8d3E6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_short_ans_clean_df(path, task='both', example_id=False):\n",
        "  return (db.read_text(path)\n",
        "           .map(json.loads)\n",
        "           .filter(lambda json_obj: has_long_answer(\n",
        "             json_obj['annotations'][0]['long_answer']\n",
        "           ))\n",
        "           .map(lambda json_obj: data_cleaning_for_short_answer(\n",
        "            json_obj,\n",
        "            task,\n",
        "            example_id   \n",
        "           ))\n",
        "           .to_dataframe()\n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGIYGqWyhHvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "14f19090-c301-456b-c21a-81180f105e9d"
      },
      "source": [
        "%%timeit -n1\n",
        "raw_df = get_short_ans_clean_df(path)\n",
        "\n",
        "print(len(raw_df))\n",
        "print(raw_df.columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152148\n",
            "Index(['question_text', 'long_answer_text', 'short_answer_start_token',\n",
            "       'short_answer_end_token', 'yes_no_answer'],\n",
            "      dtype='object')\n",
            "152148\n",
            "Index(['question_text', 'long_answer_text', 'short_answer_start_token',\n",
            "       'short_answer_end_token', 'yes_no_answer'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7j2aHzIGICy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = (db.read_text(path)\n",
        "          .map(json.loads)\n",
        "          .filter(lambda json_obj: has_long_answer(\n",
        "            json_obj['annotations'][0]['long_answer']\n",
        "          ))\n",
        "          .map(lambda json_obj: data_cleaning_for_short_answer(\n",
        "            json_obj,\n",
        "            task,\n",
        "            example_id   \n",
        "          ))\n",
        "          .take(10)\n",
        "        )\n",
        "\n",
        "for instance in data:\n",
        "  pprint.pprint(instance)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}