# Google's Natural Questions

## Datasets

- [Google's Natural Questions](https://ai.google.com/research/NaturalQuestions)

## Readings

- BERT
  - [GitHub repo](https://github.com/google-research/bert)
  - [Paper Dissected: “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” Explained](https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)
  - [Bert Baseline for NQ](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint)
  - [進擊的 BERT：NLP 界的巨人之力與遷移學習](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)
  - [ELMO, BERT, GPT](https://www.youtube.com/watch?v=UYPa347-DdE)
  - [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
  - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
  - [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)
  - [BertViz](https://github.com/jessevig/bertviz)
- ALBERT
  - [GitHub repo](https://github.com/kpe/bert-for-tf2)
  - [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
